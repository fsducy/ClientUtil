# mongo
mongo {
  cluster="192.168.1.120:27017"
  db="db"
  col="col"
}

# kafka
consumer {
  bootstrap.servers="192.168.1.104:9093,192.168.1.125:9093,192.168.1.126:9093,192.168.1.127:9093,192.168.1.129:9093"
  enable.auto.commit="true"
  auto.commit.interval.ms="1000"
  session.timeout.ms="30000"
  key.deserializer="org.apache.kafka.common.serialization.StringDeserializer"
  value.deserializer="org.apache.kafka.common.serialization.StringDeserializer"
  #auto.offset.reset="earliest"
  auto.offset.reset="latest"
}
producer {
  bootstrap.servers="192.168.1.104:9093,192.168.1.125:9093,192.168.1.126:9093,192.168.1.127:9093,192.168.1.129:9093"
  acks="all"
  retries="3"
  batch.size="16384"
  linger.ms="50"
  buffer.memory="67108864"
  max.request.size="52428800"
  compression.type="gzip"
  key.serializer="org.apache.kafka.common.serialization.StringSerializer"
  value.serializer="org.apache.kafka.common.serialization.StringSerializer"
}
kafka.topic {
  original.name="common-original"
  original.group="common-original-001"
}

# hbase
hbase {
  zookeeper.property.clientPort="2181"
  zookeeper.quorum="datanode03,datanode05,datanode04"
  table.name="table"
  table.family="family"
}

#druid
druid {
  host=datanode08
  port=28082
  endpoint=druid/v2/
  dataSource=adEffectDataSource
}

# hive
hive {
  url="jdbc:hive2://datanode02:10000/"
}

# elasticsearch
elasticsearch {
  transportaddress="192.168.1.120:9300,192.168.1.121:9300,192.168.1.122:9300,192.168.1.123:9300,192.168.1.124:9300"
  bulkactions=1000
  flushinterval=5
  index="labeled_page_db"
  type="urls"
}

# reids
redis {
  ip="192.168.1.70"
  port=6379
  db=0
  maxActive=50
  maxIdle=20
  testOnBorrow=true
  testOnReturn=true
  maxWait=3000
  timeout=3000
}